## ADAM

In this repository, we'll put our research and code on the ADAM method 
thanks to the article : [ADAM: A METHOD FOR STOCHASTIC 
OPTIMIZATION](https://browse.arxiv.org/pdf/1412.6980.pdf)

## Author
CAPEL Alexandre & BERNARD Anne, students at University of Montpellier

## ADAM

ADAM is a method for efficient stochastic optimization. 

Lors d'une descente de gradient, il est parfois difficile de trouver le 
bon taux d'apprentissage. En effet, la plupart du temps ce taux est fixe 
et s'il est trop petit cela peut poser des difficultés à trouver le 
minimum puisque la méthode n'avancera pas assez et elle peut s'arrêter 
avant de l'atteindre. S'il est trop grand nous avancerons vite mais il est 
possible de tourner autour du point sans l'atteindre car le pas est trop 
grand. Il faut donc trouver le pas parfait afin de trouver le minimum. 

